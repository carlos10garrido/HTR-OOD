{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:14: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:64: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE TOKENIZER: 94\n",
      "COMPLETE VOCAB: {'[BOS]': 0, '[EOS]': 1, '[PAD]': 2, '[UNK]': 3, ' ': 4, '!': 5, '\"': 6, '#': 7, '%': 8, '&': 9, \"'\": 10, '(': 11, ')': 12, '*': 13, '+': 14, ',': 15, '-': 16, '.': 17, '/': 18, '0': 19, '1': 20, '2': 21, '3': 22, '4': 23, '5': 24, '6': 25, '7': 26, '8': 27, '9': 28, ':': 29, ';': 30, '<': 31, '=': 32, '>': 33, '?': 34, 'A': 35, 'B': 36, 'C': 37, 'D': 38, 'E': 39, 'F': 40, 'G': 41, 'H': 42, 'I': 43, 'J': 44, 'K': 45, 'L': 46, 'M': 47, 'N': 48, 'O': 49, 'P': 50, 'Q': 51, 'R': 52, 'S': 53, 'T': 54, 'U': 55, 'V': 56, 'W': 57, 'X': 58, 'Y': 59, 'Z': 60, '[': 61, ']': 62, '_': 63, 'a': 64, 'b': 65, 'c': 66, 'd': 67, 'e': 68, 'f': 69, 'g': 70, 'h': 71, 'i': 72, 'j': 73, 'k': 74, 'l': 75, 'm': 76, 'n': 77, 'o': 78, 'p': 79, 'q': 80, 'r': 81, 's': 82, 't': 83, 'u': 84, 'v': 85, 'w': 86, 'x': 87, 'y': 88, 'z': 89, '{': 90, '}': 91, '£': 92, '€': 93}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import ImageFont, Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "\n",
    "# %%\n",
    "fonts_path = '../data/synth/final_fonts_rendered/'\n",
    "fonts = [fonts_path + f for f in os.listdir(fonts_path)]\n",
    "\n",
    "from src.data.htr_datamodule import HTRDataModule\n",
    "from src.data.htr_datamodule import HTRDataset\n",
    "import src\n",
    "import unidecode\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "tokenizer = src.data.components.tokenizers.CharTokenizer(model_name=\"char_tokenizer\", vocab_file=\"../data/vocab.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([transforms.ToImageTensor(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files ['Seite0001', 'Seite0002', 'Seite0003', 'Seite0004', 'Seite0005', 'Seite0007', 'Seite0008', 'Seite0009', 'Seite0010', 'Seite0011', 'Seite0012', 'Seite0013', 'Seite0014', 'Seite0015', 'Seite0016', 'Seite0017', 'Seite0021', 'Seite0022', 'Seite0023', 'Seite0024', 'Seite0025', 'Seite0026', 'Seite0027', 'Seite0028', 'Seite0029', 'Seite0030', 'Seite0031', 'Seite0032', 'Seite0033', 'Seite0034', 'Seite0035', 'Seite0036', 'Seite0037', 'Seite0038', 'Seite0039', 'Seite0040', 'Seite0041', 'Seite0042', 'Seite0043', 'Seite0044', 'Seite0045', 'Seite0046', 'Seite0047', 'Seite0048', 'Seite0049', 'Seite0050', 'Seite0051', 'Seite0052', 'Seite0053', 'Seite0054', 'Seite0055', 'Seite0056', 'Seite0057', 'Seite0058', 'Seite0059', 'Seite0060', 'Seite0061', 'Seite0062', 'Seite0063', 'Seite0064', 'Seite0065', 'Seite0066', 'Seite0067', 'Seite0068', 'Seite0069', 'Seite0070', 'Seite0071', 'Seite0072', 'Seite0073', 'Seite0074', 'Seite0075', 'Seite0076', 'Seite0077', 'Seite0078', 'Seite0079', 'Seite0080', 'Seite0081', 'Seite0082', 'Seite0083', 'Seite0084', 'Seite0085', 'Seite0086', 'Seite0087', 'Seite0088', 'Seite0089', 'Seite0090', 'Seite0091', 'Seite0092', 'Seite0093', 'Seite0094', 'Seite0095', 'Seite0096', 'Seite0097', 'Seite0098', 'Seite0099', 'Seite0100', 'Seite0101', 'Seite0102', 'Seite0103', 'Seite0104', 'Seite0105', 'Seite0106', 'Seite0107', 'Seite0108', 'Seite0109', 'Seite0110', 'Seite0111', 'Seite0112', 'Seite0113', 'Seite0114', 'Seite0115', 'Seite0116', 'Seite0117', 'Seite0118', 'Seite0119', 'Seite0120', 'Seite0121', 'Seite0122', 'Seite0123', 'Seite0124', 'Seite0125', 'Seite0126', 'Seite0127', 'Seite0128', 'Seite0129', 'Seite0130', 'Seite0131', 'Seite0132', 'Seite0133', 'Seite0134', 'Seite0135', 'Seite0136', 'Seite0137', 'Seite0138', 'Seite0139', 'Seite0140', 'Seite0141', 'Seite0142', 'Seite0143', 'Seite0144', 'Seite0145', 'Seite0146', 'Seite0147', 'Seite0148', 'Seite0149', 'Seite0150', 'Seite0151', 'Seite0152', 'Seite0153', 'Seite0154', 'Seite0155', 'Seite0156', 'Seite0157', 'Seite0158', 'Seite0159', 'Seite0160', 'Seite0161', 'Seite0162', 'Seite0163', 'Seite0164', 'Seite0165', 'Seite0166', 'Seite0167', 'Seite0168', 'Seite0169', 'Seite0170', 'Seite0171', 'Seite0172', 'Seite0173', 'Seite0174', 'Seite0175', 'Seite0176', 'Seite0177', 'Seite0178', 'Seite0179', 'Seite0180', 'Seite0181', 'Seite0182', 'Seite0183', 'Seite0184', 'Seite0185', 'Seite0186', 'Seite0187', 'Seite0188', 'Seite0189', 'Seite0190', 'Seite0191', 'Seite0192', 'Seite0193', 'Seite0194', 'Seite0195', 'Seite0196', 'Seite0197', 'Seite0198', 'Seite0199', 'Seite0200', 'Seite0201', 'Seite0202', 'Seite0203', 'Seite0204', 'Seite0205', 'Seite0206', 'Seite0207', 'Seite0208', 'Seite0209', 'Seite0210', 'Seite0211', 'Seite0212', 'Seite0213', 'Seite0214', 'Seite0215', 'Seite0216', 'Seite0217', 'Seite0218', 'Seite0219', 'Seite0220', 'Seite0221', 'Seite0222', 'Seite0223', 'Seite0224', 'Seite0225', 'Seite0226', 'Seite0227', 'Seite0228', 'Seite0229', 'Seite0230', 'Seite0231', 'Seite0232', 'Seite0233', 'Seite0234', 'Seite0235', 'Seite0236', 'Seite0237', 'Seite0238', 'Seite0239', 'Seite0240', 'Seite0241', 'Seite0242', 'Seite0243', 'Seite0244', 'Seite0245', 'Seite0246', 'Seite0247', 'Seite0248', 'Seite0249', 'Seite0250', 'Seite0251', 'Seite0252', 'Seite0253', 'Seite0254', 'Seite0255', 'Seite0256', 'Seite0257', 'Seite0258', 'Seite0259', 'Seite0260', 'Seite0261', 'Seite0262', 'Seite0263', 'Seite0264', 'Seite0265', 'Seite0266', 'Seite0267', 'Seite0268', 'Seite0269', 'Seite0270', 'Seite0271', 'Seite0272', 'Seite0273', 'Seite0274', 'Seite0275', 'Seite0276', 'Seite0277', 'Seite0278', 'Seite0279', 'Seite0280', 'Seite0281', 'Seite0282', 'Seite0283', 'Seite0284', 'Seite0285', 'Seite0286', 'Seite0287', 'Seite0288', 'Seite0289', 'Seite0290', 'Seite0291', 'Seite0292', 'Seite0293', 'Seite0294', 'Seite0295', 'Seite0296', 'Seite0297', 'Seite0298', 'Seite0299', 'Seite0300', 'Seite0301', 'Seite0302', 'Seite0303', 'Seite0304', 'Seite0305', 'Seite0306', 'Seite0307', 'Seite0308', 'Seite0309', 'Seite0310', 'Seite0311', 'Seite0312', 'Seite0313', 'Seite0314', 'Seite0315', 'Seite0316', 'Seite0317', 'Seite0318', 'Seite0319', 'Seite0320', 'Seite0321', 'Seite0322', 'Seite0323', 'Seite0324', 'Seite0325', 'Seite0326', 'Seite0327', 'Seite0328', 'Seite0329', 'Seite0330', 'Seite0331', 'Seite0332', 'Seite0333', 'Seite0334', 'Seite0335', 'Seite0336', 'Seite0337', 'Seite0338', 'Seite0339', 'Seite0340', 'Seite0341', 'Seite0342', 'Seite0343', 'Seite0344', 'Seite0345', 'Seite0346', 'Seite0347', 'Seite0348', 'Seite0349', 'Seite0350', 'Seite0351', 'Seite0352', 'Seite0353', 'Seite0354']\n",
      "Files ['Seite0355', 'Seite0356', 'Seite0357', 'Seite0358', 'Seite0359', 'Seite0360', 'Seite0361', 'Seite0362', 'Seite0363', 'Seite0364', 'Seite0365', 'Seite0366', 'Seite0367', 'Seite0368', 'Seite0369', 'Seite0370', 'Seite0371', 'Seite0372', 'Seite0373', 'Seite0374', 'Seite0375', 'Seite0376', 'Seite0377', 'Seite0378', 'Seite0379', 'Seite0380', 'Seite0381', 'Seite0382', 'Seite0383', 'Seite0384', 'Seite0385', 'Seite0386', 'Seite0387', 'Seite0388', 'Seite0389', 'Seite0390', 'Seite0391', 'Seite0392', 'Seite0393', 'Seite0394', 'Seite0395', 'Seite0396', 'Seite0397', 'Seite0398', 'Seite0399', 'Seite0400', 'Seite0401', 'Seite0402', 'Seite0403', 'Seite0404']\n",
      "Files ['Seite0405', 'Seite0406', 'Seite0407', 'Seite0408', 'Seite0409', 'Seite0410', 'Seite0411', 'Seite0412', 'Seite0413', 'Seite0414', 'Seite0415', 'Seite0416', 'Seite0417', 'Seite0418', 'Seite0419', 'Seite0420', 'Seite0421', 'Seite0422', 'Seite0423', 'Seite0424', 'Seite0425', 'Seite0426', 'Seite0427', 'Seite0428', 'Seite0429', 'Seite0430', 'Seite0431', 'Seite0432', 'Seite0433', 'Seite0434', 'Seite0435', 'Seite0436', 'Seite0437', 'Seite0438', 'Seite0439', 'Seite0440', 'Seite0441', 'Seite0442', 'Seite0443', 'Seite0444', 'Seite0445', 'Seite0446', 'Seite0447', 'Seite0448', 'Seite0449', 'Seite0450', 'Seite0451', 'Seite0452', 'Seite0453', 'Seite0454']\n"
     ]
    }
   ],
   "source": [
    "def get_texts(images_path, sequences_path, split_path, read_data, tokenizer, transform=v2.Compose([v2.ToTensor()])):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        setfiles = f.read().splitlines()\n",
    "    \n",
    "    images_paths, sentences = read_data(images_path, sequences_path, setfiles)\n",
    "    return images_paths, sentences\n",
    "\n",
    "# %%\n",
    "train_datasets = dict({\n",
    "  'iam': get_texts(\"../data/htr_datasets/IAM/IAM_lines/\", \"../data/htr_datasets/IAM/IAM_xml/\", \"../data/htr_datasets/IAM/splits/train.txt\", src.data.data_utils.read_data_IAM, tokenizer),\n",
    "  'rimes': get_texts(\"../data/htr_datasets/RIMES/RIMES-2011-Lines/Images/\", \"../data/htr_datasets/RIMES/RIMES-2011-Lines/Transcriptions/\", \"../data/htr_datasets/RIMES/RIMES-2011-Lines/Sets/train.txt\", src.data.data_utils.read_data_rimes, tokenizer),\n",
    "  'washington': get_texts(\"../data/htr_datasets/washington/washingtondb-v1.0/data/line_images_normalized/\", \"../data/htr_datasets/washington/washingtondb-v1.0/ground_truth/\", \"../data/htr_datasets/washington/washingtondb-v1.0/sets/cv1/train.txt\", src.data.data_utils.read_data_washington, tokenizer),\n",
    "  'saint_gall': get_texts(\"../data/htr_datasets/saint_gall/saintgalldb-v1.0/data/line_images_normalized/\", \"../data/htr_datasets/saint_gall/saintgalldb-v1.0/ground_truth/\", \"../data/htr_datasets/saint_gall/saintgalldb-v1.0/sets/train.txt\", src.data.data_utils.read_data_saint_gall, tokenizer),\n",
    "  'bentham': get_texts(\"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Images/Lines/\", \"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Transcriptions/\", \"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Partitions/train.txt\", src.data.data_utils.read_data_bentham, tokenizer),\n",
    "  'rodrigo': get_texts(\"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/images/\", \"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/text/\", \"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/partitions/train.txt\", src.data.data_utils.read_data_rodrigo, tokenizer),\n",
    "  'icfhr_2016': get_texts(\"../data/htr_datasets/icfhr_2016/lines/\", \"../data/htr_datasets/icfhr_2016/transcriptions/\", \"../data/htr_datasets/icfhr_2016/partitions/train.txt\", src.data.data_utils.read_data_icfhr_2016, tokenizer),\n",
    "})\n",
    "\n",
    "val_datasets = dict({\n",
    "  'iam': get_texts(\"../data/htr_datasets/IAM/IAM_lines/\", \"../data/htr_datasets/IAM/IAM_xml/\", \"../data/htr_datasets/IAM/splits/val.txt\", src.data.data_utils.read_data_IAM, tokenizer),\n",
    "  'rimes': get_texts(\"../data/htr_datasets/RIMES/RIMES-2011-Lines/Images/\", \"../data/htr_datasets/RIMES/RIMES-2011-Lines/Transcriptions/\", \"../data/htr_datasets/RIMES/RIMES-2011-Lines/Sets/val.txt\", src.data.data_utils.read_data_rimes, tokenizer),\n",
    "  'washington': get_texts(\"../data/htr_datasets/washington/washingtondb-v1.0/data/line_images_normalized/\", \"../data/htr_datasets/washington/washingtondb-v1.0/ground_truth/\", \"../data/htr_datasets/washington/washingtondb-v1.0/sets/cv1/val.txt\", src.data.data_utils.read_data_washington, tokenizer),\n",
    "  'saint_gall': get_texts(\"../data/htr_datasets/saint_gall/saintgalldb-v1.0/data/line_images_normalized/\", \"../data/htr_datasets/saint_gall/saintgalldb-v1.0/ground_truth/\", \"../data/htr_datasets/saint_gall/saintgalldb-v1.0/sets/val.txt\", src.data.data_utils.read_data_saint_gall, tokenizer),\n",
    "  'bentham': get_texts(\"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Images/Lines/\", \"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Transcriptions/\", \"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Partitions/val.txt\", src.data.data_utils.read_data_bentham, tokenizer),\n",
    "  'rodrigo': get_texts(\"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/images/\", \"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/text/\", \"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/partitions/val.txt\", src.data.data_utils.read_data_rodrigo, tokenizer),\n",
    "  'icfhr_2016': get_texts(\"../data/htr_datasets/icfhr_2016/lines/\", \"../data/htr_datasets/icfhr_2016/transcriptions/\", \"../data/htr_datasets/icfhr_2016/partitions/val.txt\", src.data.data_utils.read_data_icfhr_2016, tokenizer),\n",
    "})\n",
    "\n",
    "test_datasets = dict({\n",
    "  'iam': get_texts(\"../data/htr_datasets/IAM/IAM_lines/\", \"../data/htr_datasets/IAM/IAM_xml/\", \"../data/htr_datasets/IAM/splits/test.txt\", src.data.data_utils.read_data_IAM, tokenizer),\n",
    "  'rimes': get_texts(\"../data/htr_datasets/RIMES/RIMES-2011-Lines/Images/\", \"../data/htr_datasets/RIMES/RIMES-2011-Lines/Transcriptions/\", \"../data/htr_datasets/RIMES/RIMES-2011-Lines/Sets/test.txt\", src.data.data_utils.read_data_rimes, tokenizer),\n",
    "  'washington': get_texts(\"../data/htr_datasets/washington/washingtondb-v1.0/data/line_images_normalized/\", \"../data/htr_datasets/washington/washingtondb-v1.0/ground_truth/\", \"../data/htr_datasets/washington/washingtondb-v1.0/sets/cv1/test.txt\", src.data.data_utils.read_data_washington, tokenizer),\n",
    "  'saint_gall': get_texts(\"../data/htr_datasets/saint_gall/saintgalldb-v1.0/data/line_images_normalized/\", \"../data/htr_datasets/saint_gall/saintgalldb-v1.0/ground_truth/\", \"../data/htr_datasets/saint_gall/saintgalldb-v1.0/sets/test.txt\", src.data.data_utils.read_data_saint_gall, tokenizer),\n",
    "  'bentham': get_texts(\"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Images/Lines/\", \"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Transcriptions/\", \"../data/htr_datasets/bentham/BenthamDatasetR0-GT/Partitions/test.txt\", src.data.data_utils.read_data_bentham, tokenizer),\n",
    "  'rodrigo': get_texts(\"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/images/\", \"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/text/\", \"../data/htr_datasets/rodrigo/Rodrigo corpus 1.0.0/partitions/test.txt\", src.data.data_utils.read_data_rodrigo, tokenizer),\n",
    "  'icfhr_2016': get_texts(\"../data/htr_datasets/icfhr_2016/lines/\", \"../data/htr_datasets/icfhr_2016/transcriptions/\", \"../data/htr_datasets/icfhr_2016/partitions/test.txt\", src.data.data_utils.read_data_icfhr_2016, tokenizer),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "from PIL import ImageColor\n",
    "from typing import Tuple\n",
    "\n",
    "def get_text_width(image_font: ImageFont, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Get the width of a string when rendered with a given font\n",
    "    \"\"\"\n",
    "    return round(image_font.getlength(text) + 2)\n",
    "\n",
    "\n",
    "def get_text_height(image_font: ImageFont, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Get the height of a string when rendered with a given font\n",
    "    \"\"\"\n",
    "    left, top, right, bottom = image_font.getbbox(text)\n",
    "    # print(f'Top: {top}, Bottom: {bottom}')\n",
    "    return bottom\n",
    "  \n",
    "  \n",
    "def get_max_height(image_font: ImageFont, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Get the height of a string when rendered with a given font\n",
    "    \"\"\"\n",
    "    left, top, right, bottom = image_font.getbbox(text)\n",
    "    # print(f'Top: {top}, Bottom: {bottom}')\n",
    "    return round(int(bottom) - int(top))\n",
    "  \n",
    "def get_bboxes(image: Image) -> Tuple:\n",
    "    \"\"\"\n",
    "    Get the bounding boxes for the image at a pixel level\n",
    "    \"\"\"\n",
    "    image = image.convert('L') # Convert to grayscale\n",
    "    image = np.array(image)\n",
    "    # Binarize image with a threshold of 128. If the pixel value is greater than 128, set it to 255\n",
    "    image = np.where(image > 128, 255, 0)\n",
    "    \n",
    "    # print(f'Image shape: {image.shape}')\n",
    "    # Get the bounding box for the image\n",
    "    # print(f'Image shape: {image.shape}')\n",
    "    # bbox = np.where(image <= 255)\n",
    "    bbox = np.where(image < 255)\n",
    "    \n",
    "    x_min, x_max = np.min(bbox[1]), np.max(bbox[1])\n",
    "    y_min, y_max = np.min(bbox[0]), np.max(bbox[0])\n",
    "    \n",
    "    bbox = (x_min, y_min, x_max, y_max) # (left, top, right, bottom)\n",
    "    \n",
    "    return bbox\n",
    "\n",
    "\n",
    "\n",
    "def generate_line(font, text, font_size, stroke_width=0, stroke_fill=\"#000000\"):\n",
    "    # font = ImageFont.FreeTypeFont(font, font_size)\n",
    "    font = ImageFont.truetype(font, font_size)\n",
    "    bbox = font.getbbox(text)\n",
    "    \n",
    "    img_size = (int((bbox[2] - bbox[0]) * 2.7), int((bbox[3] - bbox[1]) * 2.7))\n",
    "    \n",
    "    # Check that img_size is > 0 in both dimensions\n",
    "    # print(f'Img size: {img_size}')\n",
    "    assert img_size[0] > 0 and img_size[1] > 0, f'Image size: {img_size} with font size: {font_size} and font: {font}'\n",
    "    \n",
    "    img = Image.new('RGB', img_size, color = (255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    draw.text((img_size[0]//10, img_size[1]//10), text, font=font, fill=(0, 0, 0), stroke_width=stroke_width, stroke_fill=stroke_fill)\n",
    "    \n",
    "    # print(f'Bbox with numpy: {get_bboxes(img)}')\n",
    "    \n",
    "    bbox = get_bboxes(img)\n",
    "    image = img.crop(bbox).convert('L')\n",
    "    \n",
    "    # Generate white image    \n",
    "    bboxes_chars, generated_chars = [], []\n",
    "    max_width, max_height, min_width, min_height = 0, 0, 10000, 10000\n",
    "    \n",
    "    for char in text:\n",
    "      img = Image.new(\"RGB\", (img_size[0], img_size[1]), color=(255,255,255))\n",
    "      draw = ImageDraw.Draw(img)\n",
    "      draw.text((img_size[0]//10, img_size[1]//10), char, font=font, fill=(0,0,0), stroke_width=stroke_width, stroke_fill=stroke_fill)\n",
    "      \n",
    "      if char != ' ':\n",
    "        bbox = get_bboxes(img)\n",
    "        \n",
    "        # print(f'Bbox: {bbox} for char: {char}')\n",
    "        # Check that bbox is not empty\n",
    "        assert bbox != (0, 0, 0, 0), f'Bbox is empty for char: {char}'\n",
    "        # Check that all values are positive and that the width and height are greater than 0\n",
    "        assert bbox[0] >= 0 and bbox[1] >= 0 and bbox[2] >= 0 and bbox[3] >= 0, f'Bbox is negative for char: {char} and font: {font}'\n",
    "        # Check that the width and height are greater than 0\n",
    "        assert bbox[2] - bbox[0] > 0 and bbox[3] - bbox[1] > 0, f'Bbox is negative for char: {char} and font: {font}'\n",
    "        bboxes_chars.append(bbox)\n",
    "\n",
    "        w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        # print(f'Width, height: {w}, {h}')\n",
    "        max_width = max(max_width, w)\n",
    "        max_height = max(max_height, bbox[3])\n",
    "        min_width = min(min_width, w)\n",
    "        min_height = min(min_height, bbox[1])\n",
    "        data = np.array(img)\n",
    "        generated_chars.append(data)\n",
    "      else:\n",
    "        bbox = (0, 0, 0, 0)\n",
    "        bboxes_chars.append(bbox)\n",
    "        data = np.array(img)\n",
    "        generated_chars.append(data)\n",
    "      \n",
    "    # Iterate and reescale each character according to the max height and max width\n",
    "    reescaled_chars = []\n",
    "    \n",
    "    for gen_char, bbox in zip(generated_chars, bboxes_chars):\n",
    "      if bbox == (0, 0, 0, 0):\n",
    "        gen_char = Image.fromarray(gen_char)\n",
    "        gen_char = gen_char.resize((64, 64))\n",
    "        reescaled_chars.append(gen_char)\n",
    "        continue\n",
    "        \n",
    "        \n",
    "      char = Image.fromarray(gen_char)\n",
    "      bbox_x = (bbox[0] + (bbox[2] - bbox[0]) / 2) - max_width / 2, (bbox[0] + (bbox[2] - bbox[0]) / 2) + max_width / 2\n",
    "      bbox_y = (bbox[3] - max_height, bbox[3])\n",
    "      bbox_y = (min_height, max_height)\n",
    "\n",
    "      bbox = (bbox_x[0], bbox_y[0], bbox_x[1], bbox_y[1])\n",
    "      \n",
    "      # Bbox is left, top, right, bottom\n",
    "      char = char.crop(bbox)\n",
    "      char = char.resize((64, 64)) # WARNING, CHECK OTHER RESIZE SIZE\n",
    "      char = char.convert('L')\n",
    "      assert char.size == (64, 64), f'Char size: {char.size}'\n",
    "      \n",
    "      # Check number of dims == 2\n",
    "      assert len(np.array(char).shape) == 2, f'Char shape: {np.array(char).shape}'\n",
    "      reescaled_chars.append(char)\n",
    "      \n",
    "    \n",
    "    # print(f'Max width, max height: {max_width},{max_height}')\n",
    "      \n",
    "    return image, reescaled_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 115320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 60709/115320 [00:51<01:01, 893.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with image: ../data/htr_datasets/IAM_words/words/a01/a01-117/a01-117-05-02.png. Error message: cannot identify image file '../data/htr_datasets/IAM_words/words/a01/a01-117/a01-117-05-02.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 64238/115320 [00:53<00:29, 1745.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with image: ../data/htr_datasets/IAM_words/words/r06/r06-022/r06-022-03-05.png. Error message: cannot identify image file '../data/htr_datasets/IAM_words/words/r06/r06-022/r06-022-03-05.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115320/115320 [01:36<00:00, 1197.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 115320\n",
      "Word: sensation-hunger\n",
      "Word: travelling-carriage\n",
      "Word: counter-revolution\n",
      "Word: Administration's\n",
      "Word: over-emphasizing\n",
      "Word: Parliament-square\n",
      "Word: ticket-collector\n",
      "Word: ticket-collector\n",
      "Word: brilliantly-written\n",
      "Word: plate-and-corrugation\n",
      "Word: Marriage-Go-Round\n",
      "Word: knowledgeability\n",
      "Word: bread-and-butter\n",
      "Word: live-and-let-live\n",
      "Word: Stockton-on-Tees\n",
      "Word: incomprehensible\n",
      "Word: over-emphasizing\n",
      "Word: neuro-physiologists\n",
      "Word: over-embellished\n",
      "Word: Committee-member\n",
      "Word: unconstitutional\n",
      "Word: Lieutenant-Commander\n",
      "Word: fast-disappearing\n",
      "Word: well-established\n",
      "Word: over-emphasizing\n",
      "Word: Marriage-Go-Round\n",
      "Word: responsibilities\n",
      "Word: unconstitutional\n",
      "Word: acquaintanceship\n",
      "Word: bridegroom-to-be\n",
      "Word: once-and-for-all\n",
      "Word: fourteen-pounder\n",
      "Word: misrepresentation\n",
      "Word: richly-distilled\n",
      "Word: conversationally\n",
      "Word: under-represented\n",
      "Word: -----------------------------------------------------\n",
      "Word: Self-deprecation\n",
      "Word: Leicester-square\n",
      "Word: ticket-collector\n",
      "Word: over-emphasizing\n",
      "Word: Solicitor-General\n",
      "Word: enthusiastically\n",
      "Word: middle-income-bracket\n",
      "Word: over-simplification\n",
      "Word: court-martialled\n",
      "Word: straight-forward\n",
      "Word: character-actors\n",
      "Word: over-emphasizing\n",
      "Word: once-and-for-all\n",
      "Word: SONGS-OF-BRITAIN\n",
      "Word: characterisation\n",
      "Word: responsibilities\n",
      "Word: Leicester-square\n"
     ]
    }
   ],
   "source": [
    "# Lets read the IAM by words and plot the width and height of each word\n",
    "from tqdm import tqdm\n",
    "# Read the IAM dataset from data/htr_datasets/IAM_words/words\n",
    "# Find all images with extension .png inside the folder\n",
    "all_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"../data/htr_datasets/IAM_words/words\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            all_paths.append(os.path.join(root, file))\n",
    "            \n",
    "print(f'Number of images: {len(all_paths)}')\n",
    "\n",
    "# Read all images and get the width and height of each word\n",
    "\n",
    "widths, heights = [], []\n",
    "\n",
    "for path in tqdm(all_paths):\n",
    "    try: \n",
    "      img = Image.open(path)\n",
    "      img = img.convert('L')\n",
    "      img = np.array(img)\n",
    "      # print(f'Image shape: {img.shape}')\n",
    "      width, height = img.shape[1], img.shape[0]\n",
    "      widths.append(width)\n",
    "      heights.append(height)\n",
    "    except Exception as e:\n",
    "      print(f'Error with image: {path}. Error message: {e}')\n",
    "      \n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "\n",
    "# Get the width and height of each word\n",
    "# Plot the width and height of each word\n",
    "\n",
    "\n",
    "# %%\n",
    "# From data/htr_datasets/IAM_words/xml read all the word id 'text' and save all the words in a list\n",
    "# Read the xml files and get the text for each word\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "path = '../data/htr_datasets/IAM_words/xml/'\n",
    "def read_xml(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for word in root.iter('word'):\n",
    "        words.append(word.attrib['text'])\n",
    "        \n",
    "    return words\n",
    "  \n",
    "words, lengths = [], []\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".xml\"):\n",
    "            words.extend(read_xml(os.path.join(root, file)))\n",
    "            lengths.extend([len(word) for word in words])\n",
    "            \n",
    "            \n",
    "print(f'Number of words: {len(words)}')\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "for word in words:\n",
    "    if len(word) > 15:\n",
    "        print(f'Word: {word}')\n",
    "\n",
    "iam_words = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 139,720,784 trainable parameters\n",
      "torch.Size([32, 1, 128, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import rearrange from\n",
    "\n",
    "def get_emb(sin_inp):\n",
    "    \"\"\"\n",
    "    Gets a base embedding for one dimension with sin and cos intertwined\n",
    "    \"\"\"\n",
    "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
    "    return torch.flatten(emb, -2, -1)\n",
    "\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding1D, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f'X shape: {x.shape}')\n",
    "        # Have into account the batch size\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "      \n",
    "      \n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels, dtype_override=None):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        :param dtype_override: If set, overrides the dtype of the output embedding.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / 4) * 2)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.register_buffer(\"cached_penc\", None, persistent=False)\n",
    "        self.dtype_override = dtype_override\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "\n",
    "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
    "            return self.cached_penc\n",
    "\n",
    "        self.cached_penc = None\n",
    "        batch_size, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "        pos_y = torch.arange(y, device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x).unsqueeze(1)\n",
    "        emb_y = get_emb(sin_inp_y)\n",
    "        emb = torch.zeros(\n",
    "            (x, y, self.channels * 2),\n",
    "            device=tensor.device,\n",
    "            dtype=(\n",
    "                self.dtype_override if self.dtype_override is not None else tensor.dtype\n",
    "            ),\n",
    "        )\n",
    "        emb[:, :, : self.channels] = emb_x\n",
    "        emb[:, :, self.channels : 2 * self.channels] = emb_y\n",
    "\n",
    "        self.cached_penc = emb[None, :, :, :orig_ch].repeat(tensor.shape[0], 1, 1, 1)\n",
    "        return self.cached_penc\n",
    "\n",
    "\n",
    "class PositionalEncodingPermute2D(nn.Module):\n",
    "    def __init__(self, channels, dtype_override=None):\n",
    "        \"\"\"\n",
    "        Accepts (batchsize, ch, x, y) instead of (batchsize, x, y, ch)\n",
    "        \"\"\"\n",
    "        super(PositionalEncodingPermute2D, self).__init__()\n",
    "        self.penc = PositionalEncoding2D(channels, dtype_override)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        tensor = tensor.permute(0, 2, 3, 1)\n",
    "        enc = self.penc(tensor)\n",
    "        return enc.permute(0, 3, 1, 2)\n",
    "\n",
    "    @property\n",
    "    def org_channels(self):\n",
    "        return self.penc.org_channels\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "  def __init__(self, \n",
    "    patch_size: int = 2,\n",
    "    output_size: int = 64*64,\n",
    "    in_channels: int = 1,\n",
    "    encoder_layers: int = 3,\n",
    "    decoder_layers: int = 3,\n",
    "    d_model: int = 256,\n",
    "    hidden_dim: int = 256,\n",
    "    nheads: int = 4,\n",
    "    dropout: float = 0.1,\n",
    "    activation: str = 'relu',\n",
    "  ) -> None:\n",
    "    super(Segmenter, self).__init__()\n",
    "    \n",
    "    # Convolutional block encoder\n",
    "    self.conv_encoder = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, 8, kernel_size=(3,3), stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "      nn.Conv2d(8, 16, kernel_size=(3,3), stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "      nn.Conv2d(16, 32, kernel_size=(3,3), stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "      # nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "    )\n",
    "    \n",
    "    self.patch_size = patch_size\n",
    "    self.output_size = output_size\n",
    "    self.in_channels = in_channels\n",
    "    self.encoder_layers = encoder_layers\n",
    "    self.decoder_layers = decoder_layers\n",
    "    self.d_model = d_model\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.nheads = nheads\n",
    "    self.dropout = dropout\n",
    "    self.activation = activation\n",
    "    self.patchify = nn.Conv2d(32, d_model, kernel_size=(patch_size, patch_size), stride=self.patch_size)\n",
    "    # self.positional_encoding = PositionalEncodingPermute2D(channels=d_model)\n",
    "    self.positional_encoding = PositionalEncoding1D(d_model=d_model, dropout=dropout)\n",
    "    \n",
    "    # Transformer encoder\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nheads, dim_feedforward=hidden_dim, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
    "    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=encoder_layers)\n",
    "    \n",
    "    # Convolutional block decoder (for character segmented)\n",
    "    self.conv_decoder = nn.Sequential(\n",
    "      nn.Conv2d(1, 4, kernel_size=(3,3), stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "      nn.Conv2d(4, 4, kernel_size=(3,3), stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "      nn.Conv2d(4, 4, kernel_size=(3,3), stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    self.proj_decoder = nn.Linear(4*16*16, d_model)\n",
    "    self.positional_encoding_decoder = PositionalEncoding1D(d_model=d_model, dropout=dropout)\n",
    "    \n",
    "    # Transformer decoder\n",
    "    decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nheads, dim_feedforward=hidden_dim, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
    "    self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=decoder_layers)\n",
    "    \n",
    "    # self.upconv = nn.Sequential( # From 4, 16, 16 to 1, 64, 64\n",
    "    #   nn.ConvTranspose2d(4, 1, kernel_size=(3,3), stride=2, padding=1, output_padding=1),\n",
    "    #   nn.ReLU(),\n",
    "    #   nn.ConvTranspose2d(1, 1, kernel_size=(3,3), stride=2, padding=1, output_padding=1),\n",
    "    #   # nn.Sigmoid()\n",
    "    #   nn.ReLU()\n",
    "    # )\n",
    "    self.upconv = nn.Sequential(\n",
    "      nn.Linear(4*16*16, 4*64*64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(4*64*64, 1*64*64),\n",
    "      # nn.Sigmoid()\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    # Initialize to Xavier\n",
    "    for p in self.parameters():\n",
    "      if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    \n",
    "  def forward(self, x: torch.Tensor, chars_segmented: torch.Tensor, char_lengths: torch.Tensor) -> torch.Tensor:\n",
    "    # Encoder\n",
    "    x = self.conv_encoder(x)\n",
    "    x = self.patchify(x)\n",
    "    # x = self.positional_encoding(x)\n",
    "    \n",
    "    # Rearrange from [B, C, H, W] to [B, H*W, C]\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H*W, C)\n",
    "    x = self.positional_encoding(x) # This works instead of 2D positional encoding\n",
    "    x = self.encoder(x)\n",
    "    # print(f'X shape: {x.shape} after encoder')\n",
    "    \n",
    "    memory = x\n",
    "    \n",
    "    # Decoder\n",
    "    # chars_segmented = torch.cat([torch.zeros(chars_segmented.shape[0], 1, 1, 64, 64).to(chars_segmented.device), chars_segmented], dim=1)\n",
    "    tok_decoder = chars_segmented.reshape(chars_segmented.shape[0]*chars_segmented.shape[1], 1, 64, 64)\n",
    "    # Convolution downsmapling + projection\n",
    "    tok_decoder = self.conv_decoder(tok_decoder)\n",
    "    tok_decoder = self.proj_decoder(tok_decoder.view(tok_decoder.shape[0], -1)).contiguous()\n",
    "    input_decoder = tok_decoder.reshape(chars_segmented.shape[0], chars_segmented.shape[1], -1).contiguous()\n",
    "    # Positional encoding for the decoder\n",
    "    input_decoder = self.positional_encoding_decoder(input_decoder)\n",
    "\n",
    "    mask = torch.ones(input_decoder.shape[0], input_decoder.shape[1], input_decoder.shape[1])\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    \n",
    "    tgt_key_pad_mask  = torch.zeros(chars_segmented.shape[0], chars_segmented.shape[1])\n",
    "    for i, length in enumerate(char_lengths):\n",
    "      tgt_key_pad_mask[i, length+1:] = 1\n",
    "      \n",
    "    tgt_key_pad_mask = tgt_key_pad_mask.bool()\n",
    "    \n",
    "    # Add the padding mask to the mask\n",
    "    for i, length in enumerate(char_lengths):\n",
    "      mask[i, :, length+1:] = float('-inf') # +1 because we added a zero token at the beginning\n",
    "      \n",
    "    # print(f'Mask shape: {mask.shape}')\n",
    "    # print(f'Mask: {mask}')\n",
    "    \n",
    "    # print(f'Input decoder shape: {input_decoder.shape}')\n",
    "    # print(f'Memory shape: {memory.shape}')\n",
    "    # print(f'Mask shape: {mask.shape}')\n",
    "    # print(f'Tgt key pad mask shape: {tgt_key_pad_mask.shape}')\n",
    "    \n",
    "    \n",
    "    output = self.decoder(tgt=input_decoder, memory=memory, tgt_mask=mask, tgt_is_causal=True)#, tgt_key_padding_mask=tgt_key_pad_mask)\n",
    "    \n",
    "    # AFTER\n",
    "    output = self.upconv(output)\n",
    "    output = output.view(chars_segmented.shape[0], chars_segmented.shape[1], 1, 64, 64).contiguous()\n",
    "    \n",
    "    return output\n",
    "  \n",
    "  def greedy_decoding(self, image: torch.Tensor, max_length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Greedy decoding for the transformer\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    x = self.conv_encoder(image)\n",
    "    x = self.patchify(x)\n",
    "    # x = self.positional_encoding(x)\n",
    "    \n",
    "    # Rearrange from [B, C, H, W] to [B, H*W, C]\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H*W, C)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.encoder(x)\n",
    "    \n",
    "    memory = x\n",
    "    \n",
    "    # Decoder\n",
    "    # Initialize the output tensor for the decoder\n",
    "    output = torch.ones(image.shape[0], 1, 1, 64, 64).to(image.device)\n",
    "    \n",
    "    # Decoder\n",
    "    \n",
    "    for i in range(max_length):\n",
    "      # print(f'----- Iteration: {i} -----')\n",
    "      input_decoder = output.reshape(output.shape[0]*output.shape[1], 1, 64, 64)\n",
    "      # Convolution downsmapling + projection\n",
    "      input_decoder = self.conv_decoder(input_decoder)\n",
    "      input_decoder = self.proj_decoder(input_decoder.view(input_decoder.shape[0], -1))\n",
    "      input_decoder = input_decoder.reshape(output.shape[0], output.shape[1], -1).contiguous()\n",
    "      # Positional encoding for the decoder\n",
    "      input_decoder = self.positional_encoding_decoder(input_decoder)\n",
    "      \n",
    "      # Predict the next token\n",
    "      output_ = self.decoder(tgt=input_decoder, memory=memory)\n",
    "      # print(f'Output shape (after decoder): {output_.shape}')\n",
    "      output_ = self.upconv(output_)\n",
    "      output_ = output_.view(output.shape[0], -1, 1, 64, 64).contiguous()\n",
    "      # Select only last prediction\n",
    "      output_ = output_[:, -1].unsqueeze(1)\n",
    "      # output_ = torch.where(output_ > 0.5, 1, 0)\n",
    "      # print(f'Output shape after threshold: {output_.shape} and output shape: {output.shape}')\n",
    "      # Reshape output original shape\n",
    "      output = torch.cat([output, output_], dim=1)\n",
    "      # print(f'Output shape after cat: {output.shape}')\n",
    "      \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Create a segmenter model\n",
    "segmenter = Segmenter(patch_size=2, d_model=1024, hidden_dim=256, nheads=4, encoder_layers=4, decoder_layers=4, dropout=0.0, activation='relu')\n",
    "# print(segmenter)\n",
    "\n",
    "# Calculate the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  \n",
    "print(f'The model has {count_parameters(segmenter):,} trainable parameters')\n",
    "\n",
    "# Generate a random image\n",
    "image = torch.randn(32, 1, 128, 1024)\n",
    "print(image.shape)\n",
    "\n",
    "segmented_chars = torch.randn(32, 100, 1, 64, 64)\n",
    "\n",
    "# Pass the image through the model\n",
    "output = segmenter(image, segmented_chars, torch.tensor([10]*32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_177630/2338605909.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('segmenter.ckpt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (conv_encoder): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (patchify): Conv2d(32, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (positional_encoding): PositionalEncoding1D(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_decoder): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (proj_decoder): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (positional_encoding_decoder): PositionalEncoding1D(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (upconv): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=16384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Segmenter(patch_size=2, d_model=1024, hidden_dim=1024, nheads=8, encoder_layers=4, decoder_layers=4, dropout=0.1, activation='relu')\n",
    "# model = torch.compile(model)\n",
    "\n",
    "\n",
    "# Check if path exists and load the model\n",
    "if os.path.exists('segmenter.ckpt'):\n",
    "  model.load_state_dict(torch.load('segmenter.ckpt'))\n",
    "  print('Model loaded')\n",
    "else:\n",
    "  print('Model not loaded')\n",
    "  \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([transforms.ToImageTensor(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/7207 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 0\n"
     ]
    }
   ],
   "source": [
    "# Create dataset with IAM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = HTRDataset(\n",
    "  paths_images=all_paths,\n",
    "  words=iam_words,\n",
    "  binarize=True,\n",
    "  transform=v2.Compose([v2.Pad(20, fill=255), v2.ToTensor()])\n",
    ")\n",
    "\n",
    "# import from src.data.data_utils collate_fn\n",
    "from src.data.data_utils import collate_fn\n",
    "\n",
    "# Iterate over the dataset and print the first 10 images\n",
    "batch_size = 16\n",
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=lambda batch: collate_fn(batch, (128, 256), tokenizer.prepare_text))\n",
    "\n",
    "for iteration, (images, texts, _) in tqdm(enumerate(dataloaders), total=len(dataset)//batch_size):\n",
    "  fig, ax = plt.subplots(batch_size, 16, figsize=(20, 20))\n",
    "  images = images[:, 0, None, :, :]\n",
    "  with torch.no_grad():\n",
    "    outputs = model.greedy_decoding(images, max_length=15).detach().cpu()\n",
    "    outputs = outputs > 0.5\n",
    "  \n",
    "  # Resize outputs to (B, 20, 128, 128)\n",
    "  \n",
    "  for i in range(batch_size):\n",
    "    ax[i, 0].imshow(images[i].permute(1, 2, 0), cmap='gray')\n",
    "\n",
    "    # Plot the output\n",
    "    for j in range(15):\n",
    "      ax[i, j+1].set_xticks([])\n",
    "      ax[i, j+1].imshow(outputs[i, j].squeeze(0), cmap='gray', vmin=0, vmax=1)\n",
    "      # ax[i, j+1].axis('on')\n",
    "      \n",
    "  print(f'Saving figure {iteration}')    \n",
    "  fig.savefig(f'output_{iteration}.pdf')\n",
    "      \n",
    "  if iter == 2:\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
