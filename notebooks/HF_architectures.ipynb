{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5WSt5nrXkJW"
      },
      "source": [
        "# Comparing architectures in HuggingFace (BART, BERT, Pre/Post Norm. Layers, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6N71Hf01Xlk7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    BartForConditionalGeneration,\n",
        "    BartTokenizer,\n",
        "    BartConfig,\n",
        "    BartModel,\n",
        "    BertModel,\n",
        "    BertConfig,\n",
        "    GenerationConfig,\n",
        "    EncoderDecoderModel,\n",
        "    EncoderDecoderConfig,\n",
        "    RobertaPreLayerNormConfig,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbK9SXuCXlnU"
      },
      "source": [
        "## BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nRkX3Cb0X0mP"
      },
      "outputs": [],
      "source": [
        "bart_config = BartConfig(\n",
        "    vocab_size=50265,\n",
        "    max_position_embeddings=1024,\n",
        "    encoder_layers=12,\n",
        "    encoder_ffn_dim=4096,\n",
        "    encoder_attention_heads=16,\n",
        "    decoder_layers=12,\n",
        "    decoder_ffn_dim=4096,\n",
        "    decoder_attention_heads=16,\n",
        "    encoder_layerdrop=0.0,\n",
        "    decoder_layerdrop=0.0,\n",
        "    activation_function=\"gelu\",\n",
        "    d_model=1024,\n",
        "    dropout=0.1,\n",
        "    attention_dropout=0.0,\n",
        "    activation_dropout=0.0,\n",
        "    init_std=0.02,\n",
        "    classifier_dropout=0.0,\n",
        "    scale_embedding=False,\n",
        "    use_cache=True,\n",
        "    num_labels=3,\n",
        "    pad_token_id=1,\n",
        "    bos_token_id=0,\n",
        "    eos_token_id=2,\n",
        "    is_encoder_decoder=False,\n",
        "    decoder_start_token_id=2,\n",
        "    forced_eos_token_id=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epU-H14xYMy6",
        "outputId": "5d637842-8ebf-4041-b4b8-9e252b1cfe50"
      },
      "outputs": [],
      "source": [
        "model = BartModel(bart_config)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD2arIsqX0pB"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRsTtNf8XlpZ"
      },
      "outputs": [],
      "source": [
        "bert_config = BertConfig(\n",
        "    vocab_size=50265,\n",
        "    max_position_embeddings=1024,\n",
        "    encoder_layers=12,\n",
        "    encoder_ffn_dim=4096,\n",
        "    encoder_attention_heads=16,\n",
        "    decoder_layers=12,\n",
        "    decoder_ffn_dim=4096,\n",
        "    decoder_attention_heads=16,\n",
        "    encoder_layerdrop=0.0,\n",
        "    decoder_layerdrop=0.0,\n",
        "    activation_function=\"gelu\",\n",
        "    d_model=1024,\n",
        "    dropout=0.1,\n",
        "    attention_dropout=0.0,\n",
        "    activation_dropout=0.0,\n",
        "    init_std=0.02,\n",
        "    classifier_dropout=0.0,\n",
        "    scale_embedding=False,\n",
        "    use_cache=True,\n",
        "    num_labels=3,\n",
        "    pad_token_id=1,\n",
        "    bos_token_id=0,\n",
        "    eos_token_id=2,\n",
        "    is_encoder_decoder=True,\n",
        "    decoder_start_token_id=2,\n",
        "    forced_eos_token_id=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxm-dcpyXlre",
        "outputId": "2518c2d0-9166-4658-a13a-a73908695669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "    (position_embeddings): Embedding(1024, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSdpaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = BertModel(bert_config)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bma6UFLebpX"
      },
      "source": [
        "## Conclusions:\n",
        "- BART model uses Post-normalization layers as in the original \"Attention is All You Need\"\n",
        "- BERT model uses Pre-normalization layers thorugh Embeddding Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzftNsAeiVKj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTbJMd-TqHMI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nyo5KR7p_Pq"
      },
      "source": [
        "## Custom PositionalEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cr9qAPgiVMv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=256):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1).squeeze(1)\n",
        "        print(f'PE: {pe.shape}')\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f'X shape in forward: {x.shape}')\n",
        "        print(f'PE shape in forward: {self.pe.shape}')\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655cs2JfqDfF",
        "outputId": "c0e6bf78-4bbb-496b-8f81-1ed59df1a5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PE: torch.Size([256, 256])\n",
            "PE: torch.Size([256, 256])\n",
            "BartForConditionalGeneration(\n",
            "  (model): BartModel(\n",
            "    (shared): Embedding(100, 256, padding_idx=2)\n",
            "    (encoder): BartEncoder(\n",
            "      (embed_tokens): BartScaledWordEmbedding(100, 256, padding_idx=2)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(514, 256)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x BartEncoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): BartScaledWordEmbedding(100, 256, padding_idx=2)\n",
            "      (embed_positions): BartLearnedPositionalEmbedding(514, 256)\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x BartDecoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=256, out_features=100, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "pe_encoder, pe_decoder = PositionalEncoding(256), PositionalEncoding(256)\n",
        "\n",
        "model_config = BartConfig(\n",
        "    vocab_size=100,\n",
        "    d_model=256,\n",
        "    encoder_layers=4,\n",
        "    decoder_layers=4,\n",
        "    encoder_attention_heads=4,\n",
        "    decoder_attention_heads=4,\n",
        "    encoder_ffn_dim=1024,\n",
        "    decoder_ffn_dim=1024,\n",
        "    max_position_embeddings=512, # Will be deleted\n",
        "    activation_function='relu',\n",
        "    pad_token_id=2,\n",
        "    force_bos_token_to_be_generated=True,\n",
        "    use_cache=False,\n",
        ")\n",
        "\n",
        "model = BartForConditionalGeneration(model_config)\n",
        "print(model)\n",
        "\n",
        "# Change positional encoding to the transformer\n",
        "model.model.encoder.embed_positions = pe_encoder\n",
        "model.model.decoder.embed_positions = pe_decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJUY1xgHrVt1",
        "outputId": "9d35d7d0-60a5-45a0-ea13-42b9567bf853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BartForConditionalGeneration(\n",
            "  (model): BartModel(\n",
            "    (shared): Embedding(100, 256, padding_idx=2)\n",
            "    (encoder): BartEncoder(\n",
            "      (embed_tokens): BartScaledWordEmbedding(100, 256, padding_idx=2)\n",
            "      (embed_positions): PositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x BartEncoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): BartScaledWordEmbedding(100, 256, padding_idx=2)\n",
            "      (embed_positions): PositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x BartDecoderLayer(\n",
            "          (self_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): BartSdpaAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=256, out_features=100, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxqMBJMfqx88",
        "outputId": "2322e9f6-d9e8-403b-abd7-397871f2a274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BartEncoder(\n",
            "  (embed_tokens): BartScaledWordEmbedding(100, 256, padding_idx=2)\n",
            "  (embed_positions): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0-3): 4 x BartEncoderLayer(\n",
            "      (self_attn): BartSdpaAttention(\n",
            "        (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (activation_fn): ReLU()\n",
            "      (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model.model.encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5YvvdDyqDq3",
        "outputId": "f9c2c521-bcde-4fd0-828b-3b38229c1244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PositionalEncoding(\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model.model.encoder.embed_positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "WgpVIXggrP05",
        "outputId": "954549c5-750a-445c-a146-65102b51dbd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape in forward: torch.Size([16, 128, 1, 256])\n",
            "PE shape in forward: torch.Size([256, 256])\n",
            "Pos enc batch: torch.Size([16, 128, 128, 256])\n",
            "X shape in forward: torch.Size([16, 128, 256])\n",
            "PE shape in forward: torch.Size([256, 256])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (128) must match the size of tensor b (16) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-208-2c6b7c66452f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos_enc_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Pos enc batch: {pos_enc_batch.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0menc_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrand_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (16) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "rand_batch = torch.rand((16, 128, 1, 256)) # Sequence of 128 columns with 256 channels\n",
        "\n",
        "pos_enc_batch = model.model.encoder.embed_positions(rand_batch)\n",
        "print(f'Pos enc batch: {pos_enc_batch.shape}')\n",
        "enc_images = model.model.encoder(inputs_embeds=rand_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps7etvJurhun"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "P = model.model.encoder.embed_positions.pe.squeeze(1)\n",
        "cax = plt.matshow(P)\n",
        "plt.gcf().colorbar(cax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d0FhwZXzoZz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
