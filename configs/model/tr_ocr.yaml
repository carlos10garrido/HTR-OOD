_target_: src.models.seq2seq_module.Seq2SeqModule

# Accessing to data in configs/train_htr.yaml
datasets: ${data}
_logger: ${logger}
tokenizer: ${tokenizer}

log_val_metrics: False


optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  #2 x 10^-4 lr
  lr: 0.0002
  # weight_decay: 0.1

scheduler: null

# scheduler:
#   _target_: torch.optim.lr_scheduler.SequentialLR
#   _partial_: true
#   milestones: [4000]
#   schedulers: 
#     - _target_: torch.optim.lr_scheduler.LinearLR
#       _partial_: true
#       start_factor: 0.001
#       total_iters: 4000
#       # optimizer: scheduler.optimizer

#     - _target_: torch.optim.lr_scheduler.ExponentialLR
#       _partial_: true
#       gamma: 0.999999
#       # optimizer: scheduler.optimizer
#   verbose: true
      
net: 
  _target_: src.models.components.tr_ocr.TransformerOCR
  use_backbone: False
  patch_per_column: False
  image_size: ${data.train.train_config.img_size}
  encoder_layers: 12
  encoder_attention_heads: 8
  encoder_ffn_dim: 1024
  patch_size: 16
  d_model: 1024
  dropout: 0.1
  decoder_layers: 12
  decoder_attention_heads: 8
  decoder_ffn_dim: 1024
  activation_function: 'gelu'
  tokenizer: ${tokenizer}

compile: false
