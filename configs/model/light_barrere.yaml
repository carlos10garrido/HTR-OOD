_target_: src.models.hybrid_module.HybridModule
# Accessing to data in configs/train_htr.yaml
datasets: ${data}
_logger: ${logger}
tokenizer: ${tokenizer}


optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.0001
  # weight_decay: 0.001

scheduler: null
#   #  We use an initial learning rate of 0.001 and apply a cosine-like decay 
#   # in the final 50 epochs to finetune the parameters of the network.
#   # _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   # _partial_: true
#   # mode: min
#   # factor: 0.1
#   # patience: 10
  # _target_: torch.optim.lr_scheduler.OneCycleLR
  # _partial_: true
  # max_lr: 0.0001
  # epochs: 100 # ${trainer.max_epochs}
  # steps_per_epoch: 400 # IAM 


# scheduler:
#   _target_: torch.optim.lr_scheduler.SequentialLR
#   _partial_: true
#   milestones: [2000]
#   schedulers: 
#     - _target_: torch.optim.lr_scheduler.LinearLR
#       _partial_: true
#       start_factor: 0.0001
#       total_iters: 2000
#       # optimizer: scheduler.optimizer

#     - _target_: torch.optim.lr_scheduler.ExponentialLR
#       _partial_: true
#       gamma: 0.9999
#       # optimizer: scheduler.optimizer
      

# Scheduler is a labmda funct for a linear ramp warmup of 4000 steps and a decay of 0.9999
# scheduler:
#   _target_: torch.optim.lr_scheduler.LambdaLR
#   _partial_: true
#   lr_lambda: |
#     lambda epoch: 0.9999 ** epoch if epoch < 4000 else 
#   last_epoch: -1


net: 
  _target_: src.models.components.light_barrere.Light_Barrere
  image_size: ${data.train.train_config.img_size}
  hidden_dim: 256
  intermediate_ffn_dim: 1024
  dropout: 0.2
  n_heads: 4
  encoder_layers: 4
  decoder_layers: 4
  input_size: 128
  char_embedding_size: 256
  tokenizer: ${tokenizer}

compile: false