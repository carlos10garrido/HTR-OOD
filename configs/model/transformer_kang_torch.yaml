_target_: src.models.seq2seq_module.Seq2SeqModule

# Accessing to data in configs/train_htr.yaml
datasets: ${data}
_logger: ${logger}
tokenizer: ${tokenizer}


optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  #2 x 10^-4 lr
  lr: 0.0002
  # weight_decay: 0.1

scheduler: null

# scheduler:
#   _target_: torch.optim.lr_scheduler.SequentialLR
#   _partial_: true
#   milestones: [4000]
#   schedulers: 
#     - _target_: torch.optim.lr_scheduler.LinearLR
#       _partial_: true
#       start_factor: 0.0001
#       total_iters: 4000
#       # optimizer: scheduler.optimizer

#     - _target_: torch.optim.lr_scheduler.ExponentialLR
#       _partial_: true
#       gamma: 0.9999
#       # optimizer: scheduler.optimizer
#   verbose: true
      

net: 
  _target_: src.models.components.transformer_kang_torch.TransformerKangTorch
  use_backbone: True
  patch_per_column: True
  image_size: ${data.train.train_config.img_size}
  encoder_layers: 4
  encoder_attention_heads: 8
  encoder_ffn_dim: 1024
  patch_size: 1
  # vocab_size: 101
  d_model: 1024
  dropout: 0.1
  decoder_layers: 4
  decoder_attention_heads: 8
  decoder_ffn_dim: 1024
  activation_function: 'relu'
  tokenizer: ${tokenizer}

compile: false
