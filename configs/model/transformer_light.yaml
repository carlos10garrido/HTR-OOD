_target_: src.models.transformers_hf.HTRTransformerLitModule

# Accessing to data in configs/train_htr.yaml
datasets: ${data}
_logger: ${logger}
tokenizer: ${tokenizer}


optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.0002 # 2e-4
  # weight_decay: 0.001

# scheduler: 
#   _target_: torch.optim.lr_scheduler.StepLR
#   _partial_: true
#   step_size: 20 # Reduce the learning rate every 20 epochs
#   gamma: 0.5 # by 0.5 every time
#   verbose: true
scheduler: null
# visualize_attention: False




net: 
  _target_: src.models.components.transformer_kang.TransformerKang
  use_backbone: True
  patch_per_column: True
  image_size: ${data.train.train_config.img_size}
  encoder_layers: 4
  encoder_attention_heads: 4
  encoder_ffn_dim: 256
  # vocab_size: 101
  d_model: 256
  dropout: 0.2
  decoder_layers: 4
  decoder_attention_heads: 4
  decoder_ffn_dim: 256
  activation_function: 'relu'
  tokenizer: ${tokenizer}

compile: false
